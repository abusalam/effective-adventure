<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Audio and Prompt Recorder</title>
    <style>
        /* Basic styling for layout and appearance */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background-color: #f4f7f6;
            color: #333;
            margin: 0;
            padding: 20px;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
        }
        .container {
            width: 100%;
            max-width: 600px;
            background: #ffffff;
            border-radius: 10px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
            padding: 25px;
            box-sizing: border-box;
        }
        h1 { text-align: center; color: #111; margin-top: 0; }
        .section {
            margin-bottom: 25px;
            border-bottom: 1px solid #eee;
            padding-bottom: 25px;
        }
        .section:last-child { border-bottom: none; margin-bottom: 0; padding-bottom: 0; }
        h2 { margin-top: 0; color: #444; }
        button {
            padding: 10px 15px;
            font-size: 14px;
            font-weight: 600;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            transition: background-color 0.2s, transform 0.1s;
            margin-right: 10px;
        }
        button:active { transform: scale(0.98); }
        button:disabled { background-color: #ccc; cursor: not-allowed; }
        #recordButton { background-color: #e34040; color: white; }
        #recordButton.recording { background-color: #c0392b; animation: pulse 1.5s infinite; }
        #stopButton { background-color: #555; color: white; }
        #submitButton {
            background-color: #2ecc71;
            color: white;
            font-size: 16px;
            padding: 12px 20px;
            width: 100%;
            margin-top: 20px;
        }
        textarea {
            width: 100%;
            box-sizing: border-box;
            min-height: 100px;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 6px;
            font-size: 14px;
            margin-bottom: 10px;
        }
        audio { width: 100%; margin-top: 15px; }
        
        /* (Added) New styles for the dropdown */
        label {
            font-weight: 600;
            font-size: 14px;
            display: block;
            margin-bottom: 8px;
        }
        select {
            width: 100%;
            padding: 10px;
            font-size: 16px;
            border: 1px solid #ddd;
            border-radius: 6px;
            background-color: #fff;
        }

        #feedbackArea {
            font-family: 'Courier New', Courier, monospace;
            color: #000;
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            padding: 15px;
            border-radius: 6px;
            margin-top: 15px;
            display: none;
            white-space: pre-wrap; /* Keeps formatting */
            word-wrap: break-word;
        }
        #loadingSpinner {
            display: none;
            font-style: italic;
            color: #777;
            margin-top: 10px;
        }

        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(227, 64, 64, 0.7); }
            70% { box-shadow: 0 0 0 10px rgba(227, 64, 64, 0); }
            100% { box-shadow: 0 0 0 0 rgba(227, 64, 64, 0); }
        }
    </style>
</head>
<body>

    <div class="container">
        <h1>AI Multi-Model Recorder</h1>

        <div class="section">
            <h2>1. Record Audio</h2>
            <button id="recordButton">Start Recording</button>
            <button id="stopButton" disabled>Stop Recording</button>
            <audio id="audioPlayback" controls></audio>
        </div>

        <div class="section">
            <h2>2. Write Prompt</h2>
            <textarea id="promptTextarea" placeholder="Enter your prompt... (e.g., for Whisper, this prompt guides the transcription style or provides context.)"></textarea>
        </div>

        <div class="section">
            <h2>3. Select Model & Submit</h2>
            
            <label for="modelSelect">Select AI Model:</label>
            <select id="modelSelect">
                <option value="" disabled selected>-- Choose a model --</option>
                <option value="whisper">OpenAI - Whisper (Transcription)</option>
                <option value="gpt4o_stub">OpenAI - GPT-4o (Chat Stub)</option>
                <option value="gemini_stub">Google - Gemini 1.5 Pro (Chat Stub)</option>
            </select>
            
            <button id="submitButton" disabled>Submit Audio and Prompt</button>
            
            <div id="loadingSpinner">Contacting AI...</div>
            <div id="feedbackArea"></div>
        </div>
    </div>

    <script>
        // Get references to all our HTML elements
        const recordButton = document.getElementById('recordButton');
        const stopButton = document.getElementById('stopButton');
        const audioPlayback = document.getElementById('audioPlayback');
        const promptTextarea = document.getElementById('promptTextarea');
        const modelSelect = document.getElementById('modelSelect'); // (Added)
        const submitButton = document.getElementById('submitButton');
        const feedbackArea = document.getElementById('feedbackArea');
        const loadingSpinner = document.getElementById('loadingSpinner');

        let mediaRecorder;
        let audioChunks = [];
        let audioBlob = null;
        let audioUrl = null;

        // --- 1. Audio Recording Logic (Unchanged) ---
        recordButton.addEventListener('click', async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                mediaRecorder.addEventListener('dataavailable', event => {
                    audioChunks.push(event.data);
                });
                mediaRecorder.addEventListener('stop', () => {
                    audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    audioUrl = URL.createObjectURL(audioBlob);
                    audioPlayback.src = audioUrl;
                    audioChunks = [];
                    stream.getTracks().forEach(track => track.stop());
                    updateButtonStates();
                });
                mediaRecorder.start();
                recordButton.textContent = 'Recording...';
                recordButton.classList.add('recording');
                stopButton.disabled = false;
                recordButton.disabled = true;
                submitButton.disabled = true;
            } catch (err) {
                console.error('Error starting recording:', err);
                alert('Could not start recording. Please ensure you have given microphone permissions.');
            }
        });
        stopButton.addEventListener('click', () => {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                recordButton.textContent = 'Start Recording';
                recordButton.classList.remove('recording');
                stopButton.disabled = true;
                recordButton.disabled = false;
            }
        });
        
        // --- 3. Main AI Submission Logic (Completely Modified) ---

        submitButton.addEventListener('click', () => {
            const prompt = promptTextarea.value;
            const model = modelSelect.value;

            if (!model) {
                alert("Please select an AI model from the dropdown.");
                return;
            }
            if (!audioBlob) {
                alert("Please record audio first.");
                return;
            }
            
            // Show loading state
            setLoading(true);

            // Route to the correct API handler based on dropdown
            switch (model) {
                case 'whisper':
                    callWhisperAPI(prompt, audioBlob);
                    break;
                case 'gpt4o_stub':
                    handleGpt4oStub();
                    break;
                case 'gemini_stub':
                    handleGeminiStub();
                    break;
                default:
                    alert("Invalid model selected.");
                    setLoading(false);
            }
        });

        /**
         * SETS THE LOADING STATE
         */
        function setLoading(isLoading) {
            if (isLoading) {
                submitButton.disabled = true;
                submitButton.textContent = 'Submitting...';
                loadingSpinner.style.display = 'block';
                feedbackArea.style.display = 'none';
            } else {
                submitButton.disabled = false;
                submitButton.textContent = 'Submit Audio and Prompt';
                loadingSpinner.style.display = 'none';
            }
        }

        /**
         * HANDLER 1: OpenAI Whisper (Transcription)
         * This is a fully working example!
         */
        function callWhisperAPI(prompt, audioData) {
            const apiKey = localStorage.getItem('OPENAI_API_KEY');
            if (!apiKey) {
                alert("OpenAI API Key not found. The userscript should have asked for it.");
                setLoading(false);
                return;
            }
            
            const apiEndpoint = 'https://api.openai.com/v1/audio/transcriptions';

            const formData = new FormData();
            formData.append('file', audioData, 'recording.webm');
            formData.append('model', 'whisper-1');
            // The prompt guides the model's style and helps with specific words.
            if (prompt) {
                formData.append('prompt', prompt);
            }

            fetch(apiEndpoint, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiKey}`
                },
                body: formData
            })
            .then(response => {
                if (!response.ok) {
                    return response.json().then(err => {
                         throw new Error(`API Error (${response.status}): ${err.error.message}`);
                    });
                }
                return response.json();
            })
            .then(data => {
                console.log('Whisper Response:', data);
                feedbackArea.style.display = 'block';
                feedbackArea.textContent = data.text; // The transcription
            })
            .catch(err => {
                console.error('Error with Whisper API:', err);
                feedbackArea.style.display = 'block';
                feedbackArea.textContent = `Error: ${err.message}`;
            })
            .finally(() => {
                setLoading(false);
            });
        }

        /**
         * HANDLER 2: GPT-4o (Stub)
         * This is a non-functional stub to explain the complexity.
         */
        function handleGpt4oStub() {
            const message = `
                --- GPT-4o STUB ---
                
                This is a placeholder. Making GPT-4o work with audio in a static HTML file is very complex.
                
                Why? The 'chat/completions' API (which GPT-4o uses) expects audio to be provided as a URL, not a file upload.
                
                To make this work, you would need a server to:
                1. Receive your audio file.
                2. Upload it to a public-facing URL (like Amazon S3 or Google Cloud Storage).
                3. Send the API request to OpenAI, including your text prompt AND the new public URL of the audio.
            `;
            alert(message);
            console.warn(message);
            setLoading(false);
        }
        
        /**
         * HANDLER 3: Google Gemini (Stub)
         * This is a non-functional stub to explain the complexity.
         */
        function handleGeminiStub() {
            const message = `
                --- Gemini 1.5 Pro STUB ---
                
                This is a placeholder. The Google Gemini API for audio is a two-step process and is not feasible in this simple static page.
                
                To make this work, you would need to:
                1. Make a FIRST API call to Google's 'files.upload' endpoint to upload the audio file.
                2. Google's API returns a unique 'name' (e.g., 'files/12345abc').
                3. Make a SECOND API call to Google's 'generateContent' endpoint, sending your text prompt AND the 'name' of the file you just uploaded.
                
                This complex, multi-step process is best managed by a backend server.
            `;
            alert(message);
            console.warn(message);
            setLoading(false);
        }

        // --- Helper Function (Unchanged) ---
        function updateButtonStates() {
            if (audioBlob) {
                submitButton.disabled = false;
            } else {
                submitButton.disabled = true;
            }
        }
    </script>
</body>
</html>